{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a928678",
   "metadata": {},
   "source": [
    "### 1. Fix đường dẫn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e8c3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: d:\\NLP\\NLP-project\n",
      "SRC_DIR added to path: d:\\NLP\\NLP-project\\src\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# notebooks/ -> project/\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "\n",
    "sys.path.append(str(SRC_DIR))\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"SRC_DIR added to path:\", SRC_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cdc61f",
   "metadata": {},
   "source": [
    "### 2. Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da97c6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "\n",
    "from config_loader import load_config\n",
    "from data import (\n",
    "    read_lines, tokenize_en, tokenize_fr,\n",
    "    TranslationDataset, build_collate_fn,\n",
    "    Vocab, PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN\n",
    ")\n",
    "from model import (\n",
    "    Encoder, Decoder, Seq2Seq,\n",
    "    AttentionDecoder, Seq2SeqWithAttention\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4038bb2",
   "metadata": {},
   "source": [
    "### 3. Helper functions (vocab, decode, BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb42e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_from_itos(itos_list):\n",
    "    # Khôi phục vocab từ danh sách itos đã lưu trong checkpoint\n",
    "    v = Vocab.__new__(Vocab)  # bypass __init__\n",
    "    v.itos = list(itos_list)\n",
    "    v.stoi = {tok: i for i, tok in enumerate(v.itos)}\n",
    "    return v\n",
    "\n",
    "def safe_tok_from_idx(vocab, idx: int):\n",
    "    # Chuyển index → token, tránh lỗi ngoài phạm vi\n",
    "    if 0 <= idx < len(vocab.itos):\n",
    "        return vocab.itos[idx]\n",
    "    return UNK_TOKEN\n",
    "\n",
    "def generate_hyps_from_loader(model, dataloader, tgt_vocab, max_len=50, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    hyps, refs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, src_lens, tgt_in, tgt_out in tqdm(dataloader, desc=\"generate\"):\n",
    "            src, src_lens = src.to(device), src_lens.to(device)\n",
    "\n",
    "            # Greedy decoding trên tập đánh giá\n",
    "            preds = model.greedy_decode(\n",
    "                src, src_lens,\n",
    "                max_len=max_len,\n",
    "                sos_idx=tgt_vocab.stoi[SOS_TOKEN],\n",
    "                eos_idx=tgt_vocab.stoi[EOS_TOKEN],\n",
    "            )\n",
    "\n",
    "            # Hypotheses: câu dịch do mô hình sinh ra\n",
    "            for seq in preds:\n",
    "                tokens = []\n",
    "                for idx in seq:\n",
    "                    if idx == tgt_vocab.stoi[EOS_TOKEN]:\n",
    "                        break\n",
    "                    tok = safe_tok_from_idx(tgt_vocab, int(idx))\n",
    "                    if tok not in (PAD_TOKEN, SOS_TOKEN, EOS_TOKEN):\n",
    "                        tokens.append(tok)\n",
    "                hyps.append(\" \".join(tokens))\n",
    "\n",
    "            # References: câu đích (ground truth)\n",
    "            tgt_np = tgt_out.cpu().numpy()\n",
    "            for line in tgt_np:\n",
    "                tokens = []\n",
    "                for idx in line:\n",
    "                    if int(idx) == tgt_vocab.stoi[EOS_TOKEN]:\n",
    "                        break\n",
    "                    tok = safe_tok_from_idx(tgt_vocab, int(idx))\n",
    "                    if tok not in (PAD_TOKEN, SOS_TOKEN, EOS_TOKEN):\n",
    "                        tokens.append(tok)\n",
    "                refs.append(\" \".join(tokens))\n",
    "\n",
    "    return hyps, refs\n",
    "\n",
    "def compute_corpus_bleu(hyps, refs):\n",
    "    # Tính BLEU score ở mức corpus bằng sacreBLEU\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs], force=True)\n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5128ac7",
   "metadata": {},
   "source": [
    "### 4. Helper load file & checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_existing_file(data_dir: Path, candidates):\n",
    "    for name in candidates:\n",
    "        p = data_dir / name\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def load_checkpoint_safely(ckpt_path: Path):\n",
    "    # Load checkpoint tương thích nhiều phiên bản PyTorch\n",
    "    try:\n",
    "        return torch.load(ckpt_path, map_location=DEVICE, weights_only=True)\n",
    "    except TypeError:\n",
    "        return torch.load(ckpt_path, map_location=DEVICE)\n",
    "\n",
    "def is_attention_checkpoint(state_dict):\n",
    "    \"\"\"\n",
    "    Phân biệt checkpoint có attention hay không\n",
    "    AttentionDecoder có input rnn = emb_dim + hid_dim\n",
    "    Decoder thường: emb_dim\n",
    "    \"\"\"\n",
    "    for k, v in state_dict.items():\n",
    "        if \"decoder.rnn.weight_ih_l0\" in k:\n",
    "            return v.shape[1] > 512 # Lớn hơn emb_dim → có attention\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe2c4d",
   "metadata": {},
   "source": [
    "### 5. Load config & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77457ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test samples: 1071\n"
     ]
    }
   ],
   "source": [
    "# Load cấu hình và xác định thư mục dữ liệu\n",
    "config = load_config(PROJECT_ROOT / \"config\" / \"config.yml\")\n",
    "data_dir = PROJECT_ROOT / config[\"data\"][\"data_dir\"]\n",
    "\n",
    "test_en = pick_existing_file(\n",
    "    data_dir,\n",
    "    [\"test.en\", \"test_2018_flickr.en\", \"test_2017_flickr.en\"]\n",
    ")\n",
    "test_fr = pick_existing_file(\n",
    "    data_dir,\n",
    "    [\"test.fr\", \"test_2018_flickr.fr\", \"test_2017_flickr.fr\"]\n",
    ")\n",
    "\n",
    "if test_en is None or test_fr is None:\n",
    "    raise FileNotFoundError(\"Không tìm thấy file test\")\n",
    "\n",
    "test_src = read_lines(test_en)\n",
    "test_tgt = read_lines(test_fr)\n",
    "\n",
    "# Tokenize câu nguồn và câu đích\n",
    "test_src_tok = [tokenize_en(s) for s in test_src]\n",
    "test_tgt_tok = [tokenize_fr(s) for s in test_tgt]\n",
    "\n",
    "print(\"Loaded test samples:\", len(test_src_tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002a50c",
   "metadata": {},
   "source": [
    "### 6. Load checkpoint & rebuild model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b15b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Model: Seq2SeqWithAttention\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2SeqWithAttention(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(5893, 256, padding_idx=0)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): AttentionDecoder(\n",
       "    (embedding): Embedding(6470, 256, padding_idx=0)\n",
       "    (attention): LuongAttention()\n",
       "    (rnn): LSTM(768, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=1024, out_features=6470, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load checkpoint tốt nhất đã lưu từ quá trình train\n",
    "ckpt_path = PROJECT_ROOT / \"checkpoints\" / \"best_model.pth\"\n",
    "ckpt = load_checkpoint_safely(ckpt_path)\n",
    "\n",
    "# Khôi phục vocab từ checkpoint để đảm bảo mapping token<->index giống lúc train\n",
    "src_vocab = vocab_from_itos(ckpt[\"src_itos\"])\n",
    "tgt_vocab = vocab_from_itos(ckpt[\"tgt_itos\"])\n",
    "\n",
    "cfg = ckpt.get(\"config\", config)\n",
    "m_cfg = cfg[\"model\"]\n",
    "\n",
    "emb_dim = m_cfg[\"emb_dim\"]\n",
    "hid_dim = m_cfg[\"hid_dim\"]\n",
    "n_layers = m_cfg[\"n_layers\"]\n",
    "dropout = m_cfg[\"dropout\"]\n",
    "\n",
    "# Tự nhận diện checkpoint thuộc model có attention hay không\n",
    "use_attn = is_attention_checkpoint(ckpt[\"model_state_dict\"])\n",
    "\n",
    "# Dựng lại Encoder/Decoder đúng kiến trúc đã train để load state_dict không bị mismatch\n",
    "enc = Encoder(len(src_vocab), emb_dim, hid_dim, n_layers, dropout)\n",
    "\n",
    "if use_attn:\n",
    "    dec = AttentionDecoder(len(tgt_vocab), emb_dim, hid_dim, n_layers, dropout)\n",
    "    model = Seq2SeqWithAttention(enc, dec, DEVICE).to(DEVICE)\n",
    "    print(\"→ Model: Seq2SeqWithAttention\")\n",
    "else:\n",
    "    dec = Decoder(len(tgt_vocab), emb_dim, hid_dim, n_layers, dropout)\n",
    "    model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
    "    print(\"→ Model: Seq2Seq (no attention)\")\n",
    "\n",
    "# Nạp trọng số mô hình từ checkpoint\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.eval()    # inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae5547",
   "metadata": {},
   "source": [
    "### 7. Build DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0903137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TranslationDataset(\n",
    "    test_src_tok,\n",
    "    test_tgt_tok,\n",
    "    src_vocab,\n",
    "    tgt_vocab\n",
    ")\n",
    "\n",
    "# Collate function: padding và tạo độ dài câu cho encoder\n",
    "collate_fn = build_collate_fn(src_vocab, tgt_vocab)\n",
    "\n",
    "# DataLoader cho tập test (không shuffle để giữ thứ tự mẫu)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=cfg[\"training\"][\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698c78f",
   "metadata": {},
   "source": [
    "### 8. Generate & BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ecb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generate: 100%|██████████| 17/17 [00:07<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU: 14.50543170206696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sinh câu dịch trên tập test bằng greedy decoding\n",
    "hyps_test, refs_test = generate_hyps_from_loader(\n",
    "    model,\n",
    "    test_loader,\n",
    "    tgt_vocab,\n",
    "    max_len=50,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Tính BLEU score trên toàn bộ tập test\n",
    "test_bleu = compute_corpus_bleu(hyps_test, refs_test)\n",
    "print(\"Test BLEU:\", test_bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee71a95",
   "metadata": {},
   "source": [
    "### 9. Xem nhanh vài câu dịch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c27d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: a young man participates in a career while the subject who records it smiles .\n",
      "REF: un jeune homme participe à une course pendant que le sujet qui le filme sourit .\n",
      "HYP: jeune homme fait dans un <unk> un <unk> tandis que le qui sourit sourit sourit\n",
      "------------------------------------------------------------\n",
      "SRC: the man is scratching the back of his neck while looking for a book in a book store .\n",
      "REF: l' homme se gratte l' arrière du cou tout en cherchant un livre dans une librairie .\n",
      "HYP: homme est le arrière arrière sa épaule tandis qu' regardant un livre livre livre livre livre .\n",
      "------------------------------------------------------------\n",
      "SRC: a person wearing goggles and a hat is sled riding .\n",
      "REF: une personne portant des lunettes de protection et un chapeau fait de la luge .\n",
      "HYP: personne portant des lunettes et un chapeau fait du du .\n",
      "------------------------------------------------------------\n",
      "SRC: a girl in a pink coat and flowered goloshes sledding down a hill .\n",
      "REF: une fille avec une veste rose et des <unk> à fleurs descend le long d' une colline en luge .\n",
      "HYP: fille avec un manteau rose et des <unk> <unk> descendant pente une une .\n",
      "------------------------------------------------------------\n",
      "SRC: three girls are standing in front of a window of a building .\n",
      "REF: trois filles se tiennent devant la fenêtre d' un bâtiment .\n",
      "HYP: filles sont debout devant une fenêtre d' bâtiment bâtiment .\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# In một số ví dụ dịch để so sánh câu nguồn, tham chiếu và kết quả mô hình\n",
    "for i in range(5):\n",
    "    print(\"SRC:\", \" \".join(test_src_tok[i]))\n",
    "    print(\"REF:\", refs_test[i])\n",
    "    print(\"HYP:\", hyps_test[i])\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722e2d4",
   "metadata": {},
   "source": [
    "### 10. Lưu sample ra file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6418a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved samples to: d:\\NLP\\NLP-project\\results\\samples.txt\n"
     ]
    }
   ],
   "source": [
    "# Lưu một số ví dụ dịch (SRC / REF / HYP) ra file để phục vụ phân tích và báo cáo\n",
    "save_path = PROJECT_ROOT / \"results\" / \"samples.txt\"\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for src_tokens, ref, hyp in zip(\n",
    "        test_src_tok[:200],\n",
    "        refs_test[:200],\n",
    "        hyps_test[:200],\n",
    "    ):\n",
    "        f.write(\"SRC: \" + \" \".join(src_tokens) + \"\\n\")\n",
    "        f.write(\"REF: \" + ref + \"\\n\")\n",
    "        f.write(\"HYP: \" + hyp + \"\\n\\n\")\n",
    "\n",
    "print(\"Saved samples to:\", save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
