training:
    batch_size: 64
    num_epochs: 12
    learning_rate: 0.0007
    teacher_forcing: 0.5
    min_teacher_forcing: 0.1
    tf_decay: 0.97
    clip: 1.0

model:
    emb_dim: 256
    hid_dim: 512
    n_layers: 2
    dropout: 0.5

data:
    data_dir: "data"
    src_lang: "en"
    tgt_lang: "fr"

vocab:
    min_freq: 2
    specials: ["<pad>", "<sos>", "<eos>", "<unk>"]

paths:
    checkpoint: "checkpoints/best_model.pth"
    results: "results"
