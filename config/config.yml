training:
  batch_size: 64
  num_epochs: 12
  learning_rate: 0.001
  teacher_forcing: 0.5
  min_teacher_forcing: 0.1
  tf_decay: 0.95
  clip: 1.0

model:
  emb_dim: 256
  hid_dim: 512
  n_layers: 1
  dropout: 0

data:
  data_dir: "../data/"
  src_lang: "en"
  tgt_lang: "fr"

vocab:
  min_freq: 2
  specials: ["<pad>", "<sos>", "<eos>", "<unk>"]

paths:
  checkpoint: "../checkpoints/best_model.pth"
  results: "../results/"
